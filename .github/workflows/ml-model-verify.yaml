# VirtEngine ML Model Verification Pipeline
#
# VE-3A: Verifies SavedModel loads in Go runtime and produces deterministic outputs
#
# Jobs:
#   1. verify-savedmodel  - Verify SavedModel loads and produces deterministic outputs
#   2. verify-manifest    - Verify manifest hash matches model
#   3. inference-test     - Run inference test with known inputs

name: ML Model Verification

on:
  push:
    branches:
      - main
      - mainnet/main
    paths:
      - 'ml/training/**'
      - 'pkg/inference/**'
      - 'artifacts/models/**'
  pull_request:
    branches:
      - main
    paths:
      - 'ml/training/**'
      - 'pkg/inference/**'
  workflow_dispatch:
    inputs:
      model_path:
        description: 'Path to SavedModel (relative to repo root)'
        required: false
        default: ''
      model_version:
        description: 'Model version to verify'
        required: false
        default: ''

env:
  GO_VERSION: "1.25.5"
  PYTHON_VERSION: "3.11"
  GOWORK: "off"  # Disable workspace mode for vendor operations
  TF_VERSION: "2.15.0"
  GOWORK: "off"

permissions:
  contents: read

jobs:
  # ===========================================================================
  # Python Model Training Tests
  # ===========================================================================
  test-training-pipeline:
    name: Training Pipeline Tests
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y tesseract-ocr libtesseract-dev

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: 'ml/requirements-deterministic.txt'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r ml/requirements-deterministic.txt
          pip install pytest pytest-cov

      - name: Run training module tests
        run: |
          pytest ml/training/tests/ -v --tb=short --cov=ml/training --cov-report=term-missing
        env:
          PYTHONPATH: ${{ github.workspace }}
          PYTHONHASHSEED: 42
          TF_DETERMINISTIC_OPS: 1

      - name: Validate training config
        run: |
          python -c "
          import yaml
          from pathlib import Path
          
          config_path = Path('ml/training/configs/trust_score_v1.yaml')
          if config_path.exists():
              with open(config_path) as f:
                  config = yaml.safe_load(f)
              
              # Validate required fields
              assert 'experiment_name' in config, 'Missing experiment_name'
              assert 'random_seed' in config, 'Missing random_seed'
              assert 'model' in config, 'Missing model config'
              assert 'export' in config, 'Missing export config'
              
              # Validate determinism settings
              assert config.get('deterministic', False), 'Deterministic mode must be enabled'
              assert config.get('random_seed') == 42, 'Random seed must be 42'
              
              print('✅ Training config validation passed')
          else:
              print('⚠️ Training config not found, skipping validation')
          "

  # ===========================================================================
  # Go Inference Package Tests
  # ===========================================================================
  test-go-inference:
    name: Go Inference Tests
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: ${{ env.GO_VERSION }}
          cache: true

      - name: Download dependencies
        env:
          GOWORK: off
        run: |
          go mod tidy
          go mod vendor

      - name: Run inference package tests
        run: |
          go test -v -race ./pkg/inference/...

      - name: Run conformance tests
        run: |
          go test -v ./pkg/inference/... -run TestConformance

  # ===========================================================================
  # SavedModel Verification
  # ===========================================================================
  verify-savedmodel:
    name: Verify SavedModel
    runs-on: ubuntu-latest
    needs: [test-training-pipeline, test-go-inference]
    if: github.event_name == 'workflow_dispatch' || github.ref == 'refs/heads/main'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install TensorFlow
        run: |
          pip install tensorflow-cpu==${{ env.TF_VERSION }}

      - name: Verify SavedModel
        run: |
          python << 'EOF'
          import os
          import sys
          import json
          import hashlib
          
          # Get model path from input or environment
          model_path = os.environ.get('MODEL_PATH', '')
          
          if not model_path:
              # Look for models in standard locations
              candidates = [
                  'artifacts/models/latest/model',
                  'output/exported_models/latest/model',
              ]
              for c in candidates:
                  if os.path.exists(c):
                      model_path = c
                      break
          
          if not model_path or not os.path.exists(model_path):
              print('⚠️ No SavedModel found, skipping verification')
              print('This is expected if no model has been trained yet.')
              sys.exit(0)
          
          print(f'Verifying SavedModel at: {model_path}')
          
          import tensorflow as tf
          
          # Load model
          try:
              model = tf.saved_model.load(model_path)
              print('✅ Model loaded successfully')
          except Exception as e:
              print(f'❌ Failed to load model: {e}')
              sys.exit(1)
          
          # Check for serving signature
          if hasattr(model, 'signatures'):
              if 'serving_default' in model.signatures:
                  print('✅ serving_default signature found')
                  
                  # Get input/output specs
                  sig = model.signatures['serving_default']
                  print(f'   Input spec: {sig.structured_input_signature}')
                  print(f'   Output keys: {list(sig.structured_outputs.keys())}')
              else:
                  print('❌ serving_default signature not found')
                  sys.exit(1)
          
          # Run determinism test
          import numpy as np
          np.random.seed(42)
          
          # Create test input
          test_input = tf.constant(np.random.randn(1, 768).astype(np.float32))
          
          # Run multiple times
          outputs = []
          for i in range(3):
              result = model.signatures['serving_default'](test_input)
              if 'trust_score' in result:
                  outputs.append(result['trust_score'].numpy())
          
          # Check determinism
          if len(outputs) > 1:
              for i in range(1, len(outputs)):
                  if not np.allclose(outputs[0], outputs[i]):
                      print('❌ Model produces non-deterministic outputs!')
                      sys.exit(1)
              print('✅ Determinism verified (3 runs produced identical outputs)')
          
          print('✅ SavedModel verification complete')
          EOF
        env:
          MODEL_PATH: ${{ github.event.inputs.model_path }}
          TF_DETERMINISTIC_OPS: 1

  # ===========================================================================
  # Manifest Verification
  # ===========================================================================
  verify-manifest:
    name: Verify Manifest
    runs-on: ubuntu-latest
    needs: verify-savedmodel
    if: github.event_name == 'workflow_dispatch' || github.ref == 'refs/heads/main'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Verify manifest hash
        run: |
          python << 'EOF'
          import os
          import sys
          import json
          import hashlib
          from pathlib import Path
          
          # Find manifest
          manifest_paths = [
              'artifacts/models/latest/manifest.json',
              'output/exported_models/latest/manifest.json',
          ]
          
          manifest_path = None
          for p in manifest_paths:
              if os.path.exists(p):
                  manifest_path = p
                  break
          
          if not manifest_path:
              print('⚠️ No manifest found, skipping verification')
              sys.exit(0)
          
          print(f'Verifying manifest: {manifest_path}')
          
          with open(manifest_path) as f:
              manifest = json.load(f)
          
          # Get model path from manifest
          model_path = manifest.get('model_path', '')
          if not model_path or not os.path.exists(model_path):
              # Try relative path
              model_path = os.path.join(os.path.dirname(manifest_path), 'model')
          
          if not os.path.exists(model_path):
              print(f'⚠️ Model path not found: {model_path}')
              sys.exit(0)
          
          # Compute hash
          h = hashlib.sha256()
          files = []
          for root, dirs, filenames in os.walk(model_path):
              dirs.sort()
              for filename in sorted(filenames):
                  if filename == 'export_metadata.json':
                      continue
                  files.append(os.path.join(root, filename))
          
          for filepath in files:
              with open(filepath, 'rb') as f:
                  h.update(f.read())
          
          computed_hash = h.hexdigest()
          manifest_hash = manifest.get('model_hash', '')
          
          if computed_hash == manifest_hash:
              print(f'✅ Hash verified: {computed_hash[:16]}...')
          else:
              print(f'❌ Hash mismatch!')
              print(f'   Expected: {manifest_hash}')
              print(f'   Computed: {computed_hash}')
              sys.exit(1)
          
          # Verify evaluation passed
          if manifest.get('evaluation_passed'):
              print('✅ Evaluation thresholds passed')
          else:
              print('⚠️ Evaluation thresholds not passed (may be pre-training manifest)')
          
          print('✅ Manifest verification complete')
          EOF

  # ===========================================================================
  # Go Integration Test
  # ===========================================================================
  go-inference-integration:
    name: Go Inference Integration
    runs-on: ubuntu-latest
    needs: [verify-savedmodel, verify-manifest]
    if: github.event_name == 'workflow_dispatch' || github.ref == 'refs/heads/main'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: ${{ env.GO_VERSION }}
          cache: true

      - name: Download dependencies
        env:
          GOWORK: off
        run: |
          go mod tidy
          go mod vendor

      - name: Run SavedModel integration test
        run: |
          go test -v ./pkg/inference/... -run TestSavedModelIntegration -tags="integration"
        env:
          VEID_MODEL_PATH: ${{ github.event.inputs.model_path }}
          TF_FORCE_CPU: 1

  # ===========================================================================
  # Summary
  # ===========================================================================
  summary:
    name: Verification Summary
    runs-on: ubuntu-latest
    needs: [test-training-pipeline, test-go-inference, verify-savedmodel, verify-manifest, go-inference-integration]
    if: always()
    steps:
      - name: Generate Summary
        run: |
          echo "# ML Model Verification Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Verification Steps" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Step | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Training Pipeline Tests | ${{ needs.test-training-pipeline.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Go Inference Tests | ${{ needs.test-go-inference.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| SavedModel Verification | ${{ needs.verify-savedmodel.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Manifest Verification | ${{ needs.verify-manifest.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Go Integration Test | ${{ needs.go-inference-integration.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Model Version" >> $GITHUB_STEP_SUMMARY
          echo "- Version: ${{ github.event.inputs.model_version || 'N/A' }}" >> $GITHUB_STEP_SUMMARY
          echo "- Path: ${{ github.event.inputs.model_path || 'default' }}" >> $GITHUB_STEP_SUMMARY
