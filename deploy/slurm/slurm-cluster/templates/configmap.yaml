apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "slurm-cluster.fullname" . }}-config
  namespace: {{ .Release.Namespace }}
  labels:
    {{- include "slurm-cluster.labels" . | nindent 4 }}
data:
  slurm.conf: |
    # VirtEngine SLURM Configuration
    # Generated by Helm chart {{ .Chart.Name }}-{{ .Chart.Version }}
    
    # Cluster identification
    ClusterName={{ .Values.controller.config.clusterName | default "virtengine" }}
    SlurmctldHost={{ include "slurm-cluster.controller.serviceName" . }}-0
    
    # Authentication
    AuthType=auth/munge
    AuthAltTypes=auth/jwt
    AuthAltParameters=jwt_key=/var/spool/slurm/jwt_hs256.key
    CryptoType=crypto/munge
    
    # Scheduler configuration
    SchedulerType={{ .Values.controller.config.schedulerType | default "sched/backfill" }}
    SelectType={{ .Values.controller.config.selectType | default "select/cons_tres" }}
    SelectTypeParameters=CR_Core_Memory
    
    # Priority configuration
    PriorityType={{ .Values.controller.config.priorityType | default "priority/multifactor" }}
    PriorityWeightAge={{ .Values.controller.config.priorityWeightAge | default 1000 }}
    PriorityWeightFairshare={{ .Values.controller.config.priorityWeightFairshare | default 10000 }}
    PriorityWeightJobSize={{ .Values.controller.config.priorityWeightJobSize | default 1000 }}
    PriorityWeightPartition={{ .Values.controller.config.priorityWeightPartition | default 1000 }}
    PriorityWeightQOS={{ .Values.controller.config.priorityWeightQOS | default 10000 }}
    
    # Preemption
    PreemptMode={{ .Values.controller.config.preemptMode | default "REQUEUE" }}
    PreemptType=preempt/partition_prio
    
    # Accounting
    AccountingStorageType={{ .Values.controller.config.accountingStorageType | default "accounting_storage/slurmdbd" }}
    AccountingStorageHost={{ include "slurm-cluster.database.serviceName" . }}
    AccountingStoragePort=6819
    AccountingStoreFlags=job_comment,job_env,job_extra,job_script
    
    # Job accounting
    JobAcctGatherType={{ .Values.controller.config.jobAcctGatherType | default "jobacct_gather/cgroup" }}
    JobAcctGatherFrequency=30
    
    # Process tracking
    ProctrackType={{ .Values.controller.config.proctrackType | default "proctrack/cgroup" }}
    
    # Task management
    TaskPlugin={{ .Values.controller.config.taskPlugin | default "task/cgroup,task/affinity" }}
    
    # Logging
    SlurmctldLogFile=/var/log/slurm/slurmctld.log
    SlurmdLogFile=/var/log/slurm/slurmd.log
    SlurmctldDebug=info
    SlurmdDebug=info
    
    # Timeouts
    SlurmctldTimeout=300
    SlurmdTimeout=300
    InactiveLimit=0
    MinJobAge=300
    KillWait=30
    WaitTime=0
    
    # State directories
    StateSaveLocation=/var/spool/slurm/ctld
    SlurmdSpoolDir=/var/spool/slurm/d
    SlurmctldPidFile=/run/slurm/slurmctld.pid
    SlurmdPidFile=/run/slurm/slurmd.pid
    
    # Return to service
    ReturnToService=2
    
    # MPI support
    MpiDefault=pmix
    
    # GPU/GRES support
    GresTypes=gpu
    
    # Health check
    HealthCheckInterval=60
    HealthCheckNodeState=IDLE
    HealthCheckProgram=/usr/local/bin/slurm-health-check.sh
    
    # Nodes
    {{- $fullname := include "slurm-cluster.fullname" . }}
    {{- range $i := until (int .Values.compute.replicas) }}
    NodeName={{ $fullname }}-compute-{{ $i }} CPUs={{ $.Values.compute.config.cpus | default 4 }} RealMemory={{ $.Values.compute.config.memory | default 4096 }} State=UNKNOWN
    {{- end }}
    
    {{- range .Values.nodePools }}
    {{- range $i := until (int .replicas) }}
    NodeName={{ $fullname }}-{{ .name }}-{{ $i }} CPUs={{ .cpus | default 4 }} RealMemory={{ .memory | default 4096 }} Gres={{ if .gpus }}gpu:{{ .gpuType | default "nvidia" }}:{{ .gpus }}{{ end }} Features={{ .features | join "," }} State=UNKNOWN
    {{- end }}
    {{- end }}
    
    # Partitions
    {{- range .Values.partitions }}
    PartitionName={{ .name }} Nodes={{ .nodes | default (include "slurm-cluster.nodeList" $) }} MaxTime={{ .maxTime | default "24:00:00" }} DefaultTime={{ .defaultTime | default "1:00:00" }} MaxNodes={{ .maxNodes | default 32 }} State={{ .state | default "UP" }}{{ if .default }} Default=YES{{ end }}{{ if .priority }} PriorityTier={{ .priority }}{{ end }}
    {{- end }}

  slurmdbd.conf: |
    # VirtEngine slurmdbd Configuration
    AuthType=auth/munge
    DbdHost={{ include "slurm-cluster.database.serviceName" . }}
    DbdPort=6819
    SlurmUser=slurm
    DebugLevel=info
    LogFile=/var/log/slurm/slurmdbd.log
    PidFile=/run/slurm/slurmdbd.pid
    
    # Database connection
    StorageType=accounting_storage/mysql
    StorageHost={{ .Values.database.config.host | default (include "slurm-cluster.mariadb.serviceName" .) }}
    StoragePort={{ .Values.database.config.port | default 3306 }}
    StorageLoc={{ .Values.database.config.name | default "slurm_acct_db" }}
    StorageUser={{ .Values.database.config.user | default "slurm" }}
    # Password loaded from secret via environment variable
    
    # Archive settings
    ArchiveEvents=yes
    ArchiveJobs=yes
    ArchiveResvs=yes
    ArchiveSteps=no
    ArchiveSuspend=no
    ArchiveTXN=no
    ArchiveUsage=no
    PurgeEventAfter=1month
    PurgeJobAfter=12month
    PurgeResvAfter=1month
    PurgeStepAfter=1month
    PurgeSuspendAfter=1month
    PurgeTXNAfter=12month
    PurgeUsageAfter=24month

  cgroup.conf: |
    # VirtEngine SLURM cgroup Configuration
    CgroupMountpoint=/sys/fs/cgroup
    CgroupAutomount=yes
    ConstrainCores=yes
    ConstrainDevices=yes
    ConstrainRAMSpace=yes
    ConstrainSwapSpace=yes

  gres.conf: |
    # VirtEngine SLURM GRES (GPU) Configuration
    {{- if .Values.compute.gpu.enabled }}
    {{- range $i := until (int .Values.compute.gpu.count) }}
    Name=gpu Type={{ $.Values.compute.config.gpuType | default "nvidia" }} File=/dev/nvidia{{ $i }}
    {{- end }}
    {{- end }}

  slurm-health-check.sh: |
    #!/bin/bash
    # Health check script for SLURM compute nodes
    
    set -e
    
    # Check slurmd is running
    if ! pgrep -x slurmd > /dev/null; then
        echo "slurmd not running"
        exit 1
    fi
    
    # Check munge is running
    if ! pgrep -x munged > /dev/null; then
        echo "munged not running"
        exit 1
    fi
    
    # Check /tmp is writable
    if ! touch /tmp/.health_check_test 2>/dev/null; then
        echo "/tmp not writable"
        exit 1
    fi
    rm -f /tmp/.health_check_test
    
    # All checks passed
    exit 0
