# SCALE-002: Scaling Alert Rules
# Prometheus alerting rules for horizontal scaling monitoring
groups:
  - name: scaling.rules
    interval: 30s
    rules:
      # Provider Daemon Scaling Alerts
      - alert: ProviderDaemonHPAMaxedOut
        expr: |
          kube_horizontalpodautoscaler_status_current_replicas{
            namespace="virtengine",
            horizontalpodautoscaler=~"provider-daemon.*"
          } >= kube_horizontalpodautoscaler_spec_max_replicas{
            namespace="virtengine",
            horizontalpodautoscaler=~"provider-daemon.*"
          }
        for: 15m
        labels:
          severity: warning
          component: provider-daemon
          team: infrastructure
        annotations:
          summary: "Provider daemon HPA at maximum replicas"
          description: |
            HPA {{ $labels.horizontalpodautoscaler }} has been at maximum replicas 
            ({{ $value }}) for 15 minutes. Consider increasing max replicas.
          runbook_url: "https://docs.virtengine.network/runbooks/hpa-maxed-out"

      - alert: ProviderDaemonScalingFailure
        expr: |
          kube_horizontalpodautoscaler_status_condition{
            namespace="virtengine",
            horizontalpodautoscaler=~"provider-daemon.*",
            condition="ScalingActive",
            status="false"
          } == 1
        for: 5m
        labels:
          severity: critical
          component: provider-daemon
          team: infrastructure
        annotations:
          summary: "Provider daemon HPA scaling failure"
          description: |
            HPA {{ $labels.horizontalpodautoscaler }} is unable to scale.
            Check for resource constraints or configuration issues.
          runbook_url: "https://docs.virtengine.network/runbooks/scaling-failure"

      - alert: ProviderDaemonHighPendingOrders
        expr: |
          sum(provider_daemon_pending_orders_total) > 500
        for: 5m
        labels:
          severity: critical
          component: provider-daemon
          team: platform
        annotations:
          summary: "High number of pending orders"
          description: |
            {{ $value }} orders are pending across all provider daemon instances.
            Scaling may be needed or there could be processing issues.
          runbook_url: "https://docs.virtengine.network/runbooks/high-pending-orders"

      - alert: ProviderDaemonClaimConflictsHigh
        expr: |
          rate(provider_daemon_claim_conflicts_total[5m]) > 10
        for: 10m
        labels:
          severity: warning
          component: provider-daemon
          team: platform
        annotations:
          summary: "High rate of bid claim conflicts"
          description: |
            Bid deduplication is experiencing {{ printf "%.2f" $value }} conflicts/sec.
            This may indicate instances are competing for the same orders.
          runbook_url: "https://docs.virtengine.network/runbooks/claim-conflicts"

      - alert: ProviderDaemonBidLatencyHigh
        expr: |
          histogram_quantile(0.99, 
            sum(rate(provider_daemon_bid_duration_seconds_bucket[5m])) by (le)
          ) > 2
        for: 10m
        labels:
          severity: warning
          component: provider-daemon
          team: platform
        annotations:
          summary: "High bid processing latency"
          description: |
            P99 bid processing latency is {{ printf "%.2f" $value }}s.
            This may impact order fulfillment rates.
          runbook_url: "https://docs.virtengine.network/runbooks/bid-latency"

      # Full Node Scaling Alerts
      - alert: FullNodeHPAMaxedOut
        expr: |
          kube_horizontalpodautoscaler_status_current_replicas{
            namespace="virtengine",
            horizontalpodautoscaler=~"fullnode.*"
          } >= kube_horizontalpodautoscaler_spec_max_replicas{
            namespace="virtengine",
            horizontalpodautoscaler=~"fullnode.*"
          }
        for: 15m
        labels:
          severity: warning
          component: fullnode
          team: infrastructure
        annotations:
          summary: "Full node HPA at maximum replicas"
          description: |
            HPA {{ $labels.horizontalpodautoscaler }} has been at maximum replicas 
            for 15 minutes. RPC capacity may be constrained.
          runbook_url: "https://docs.virtengine.network/runbooks/hpa-maxed-out"

      - alert: FullNodeRPCLatencyHigh
        expr: |
          histogram_quantile(0.99, 
            sum(rate(virtengine_rpc_duration_seconds_bucket[5m])) by (le)
          ) > 0.5
        for: 10m
        labels:
          severity: warning
          component: fullnode
          team: platform
        annotations:
          summary: "High RPC query latency"
          description: |
            P99 RPC latency is {{ printf "%.2f" $value }}s.
            Consider scaling up full nodes.
          runbook_url: "https://docs.virtengine.network/runbooks/rpc-latency"

      - alert: FullNodeConnectionsHigh
        expr: |
          sum(virtengine_active_websocket_connections) > 5000
        for: 5m
        labels:
          severity: warning
          component: fullnode
          team: infrastructure
        annotations:
          summary: "High number of active connections"
          description: |
            {{ $value }} active WebSocket connections.
            May need to scale up full nodes or adjust connection limits.
          runbook_url: "https://docs.virtengine.network/runbooks/high-connections"

      # Cluster Capacity Alerts
      - alert: ClusterCapacityInsufficient
        expr: |
          sum(kube_pod_status_phase{
            phase="Pending",
            namespace="virtengine"
          }) > 0
        for: 10m
        labels:
          severity: critical
          component: cluster
          team: infrastructure
        annotations:
          summary: "Pods pending due to insufficient cluster capacity"
          description: |
            {{ $value }} pods are in Pending state in virtengine namespace.
            Cluster may need more nodes or resource constraints need adjustment.
          runbook_url: "https://docs.virtengine.network/runbooks/cluster-capacity"

      - alert: ClusterNodePressure
        expr: |
          sum(kube_node_status_condition{
            condition=~"MemoryPressure|DiskPressure|PIDPressure",
            status="true"
          }) > 0
        for: 5m
        labels:
          severity: warning
          component: cluster
          team: infrastructure
        annotations:
          summary: "Cluster nodes under resource pressure"
          description: |
            {{ $value }} nodes are experiencing resource pressure.
            Consider scaling cluster or investigating resource usage.
          runbook_url: "https://docs.virtengine.network/runbooks/node-pressure"

      # State Sync Alerts
      - alert: StateSyncProviderDown
        expr: |
          up{job="virtengine-statesync-provider"} == 0
        for: 5m
        labels:
          severity: critical
          component: statesync
          team: infrastructure
        annotations:
          summary: "State sync provider is down"
          description: |
            State sync provider {{ $labels.instance }} is not responding.
            New validators cannot sync using state sync.
          runbook_url: "https://docs.virtengine.network/runbooks/statesync-down"

      - alert: StateSyncSnapshotStale
        expr: |
          time() - virtengine_statesync_snapshot_last_created_timestamp > 7200
        for: 15m
        labels:
          severity: warning
          component: statesync
          team: infrastructure
        annotations:
          summary: "State sync snapshot is stale"
          description: |
            No new state sync snapshot created in the last 2 hours on {{ $labels.instance }}.
            New validators may sync to outdated state.
          runbook_url: "https://docs.virtengine.network/runbooks/statesync-stale"

      # Multi-Region Alerts
      - alert: RegionUnhealthy
        expr: |
          sum by (region) (
            up{job=~"virtengine-.*"}
          ) / count by (region) (
            up{job=~"virtengine-.*"}
          ) < 0.5
        for: 10m
        labels:
          severity: critical
          component: multi-region
          team: infrastructure
        annotations:
          summary: "Region {{ $labels.region }} is unhealthy"
          description: |
            Less than 50% of VirtEngine services are up in region {{ $labels.region }}.
            Consider traffic failover to healthy regions.
          runbook_url: "https://docs.virtengine.network/runbooks/region-unhealthy"

      - alert: CrossRegionLatencyHigh
        expr: |
          histogram_quantile(0.99,
            sum(rate(virtengine_cross_region_rpc_duration_seconds_bucket[5m])) by (le, source_region, target_region)
          ) > 0.5
        for: 10m
        labels:
          severity: warning
          component: multi-region
          team: infrastructure
        annotations:
          summary: "High cross-region latency"
          description: |
            Cross-region RPC latency from {{ $labels.source_region }} to 
            {{ $labels.target_region }} is {{ printf "%.2f" $value }}s.
          runbook_url: "https://docs.virtengine.network/runbooks/cross-region-latency"

  - name: scaling.recording.rules
    interval: 30s
    rules:
      # Recording rules for dashboard efficiency
      - record: provider_daemon:orders_per_instance:rate5m
        expr: |
          sum(rate(provider_daemon_orders_processed_total[5m])) by (instance)

      - record: provider_daemon:bids_success_rate:rate5m
        expr: |
          sum(rate(provider_daemon_bids_submitted_total[5m])) /
          (sum(rate(provider_daemon_bids_submitted_total[5m])) + 
           sum(rate(provider_daemon_bids_failed_total[5m])))

      - record: fullnode:rpc_requests_per_pod:rate5m
        expr: |
          sum(rate(virtengine_rpc_requests_total[5m])) by (pod)

      - record: scaling:hpa_utilization:ratio
        expr: |
          kube_horizontalpodautoscaler_status_current_replicas{namespace="virtengine"} /
          kube_horizontalpodautoscaler_spec_max_replicas{namespace="virtengine"}

      - record: scaling:cluster_cpu_headroom:ratio
        expr: |
          1 - (
            sum(rate(container_cpu_usage_seconds_total{namespace="virtengine"}[5m])) /
            sum(kube_pod_container_resource_limits{namespace="virtengine", resource="cpu"})
          )

      - record: scaling:cluster_memory_headroom:ratio
        expr: |
          1 - (
            sum(container_memory_working_set_bytes{namespace="virtengine"}) /
            sum(kube_pod_container_resource_limits{namespace="virtengine", resource="memory"})
          )
